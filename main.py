"""Provides CLI entry point for the AI assistant application."""

import asyncio
import os
from pathlib import Path
from loguru import logger

from app.core.logger import setup_logger
from app.ai.gemini import gemini_client
from app.ai.models import ChatRequest
from app.vector_db.reader import document_reader
from app.vector_db.vector_store import vector_store


async def main() -> None:
    """Runs CLI interface for AI chat application."""

    setup_logger()
    await vector_store.initialize()

    # –ù–ê–°–¢–†–û–ô–ö–ò - –º–µ–Ω—è–π –∑–¥–µ—Å—å
    DOCUMENTS_FOLDER = "./documents"  # –ü–∞–ø–∫–∞ —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
    QUESTION = "–ö–æ–≥–¥–∞ –¥–µ–Ω—å —Ä–æ–∂–¥–µ–Ω–∏–µ —É –û–ª–∞–±–∏ –î–º–∏—Ç—Ä–∏—è?"  # –¢–≤–æ–π –≤–æ–ø—Ä–æ—Å

    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ –ø–∞–ø–∫–∏
        docs_path = Path(DOCUMENTS_FOLDER)

        if not docs_path.exists():
            logger.error(f"–ü–∞–ø–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {DOCUMENTS_FOLDER}")
            logger.info("–°–æ–∑–¥–∞–π –ø–∞–ø–∫—É './documents' –∏ –ø–æ–ª–æ–∂–∏ —Ç—É–¥–∞ PDF –∏–ª–∏ TXT —Ñ–∞–π–ª—ã")
            return

        # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–∞–π–ª—ã
        supported_files = []
        for file_path in docs_path.iterdir():
            if file_path.is_file() and file_path.suffix.lower() in ['.pdf', '.txt']:
                supported_files.append(file_path)

        if not supported_files:
            logger.error(f"–í –ø–∞–ø–∫–µ {DOCUMENTS_FOLDER} –Ω–µ—Ç PDF –∏–ª–∏ TXT —Ñ–∞–π–ª–æ–≤")
            return

        logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(supported_files)} —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–∞–∂–¥—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
        total_chunks = 0
        for file_path in supported_files:
            logger.info(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é: {file_path.name}")

            # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª
            content = document_reader.read_file(str(file_path))

            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏
            chunks = document_reader.split_text(content)

            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –ë–î
            count = await vector_store.add_chunks(chunks, str(file_path))
            total_chunks += count

            logger.info(f"–î–æ–±–∞–≤–ª–µ–Ω–æ {count} —á–∞–Ω–∫–æ–≤ –∏–∑ {file_path.name}")

        logger.info(f"–í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {total_chunks} —á–∞–Ω–∫–æ–≤ –∏–∑ {len(supported_files)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")

        # –ó–∞–¥–∞–µ–º –≤–æ–ø—Ä–æ—Å —Å RAG
        logger.info(f"–ó–∞–¥–∞—é –≤–æ–ø—Ä–æ—Å: {QUESTION}")

        request = ChatRequest(message=QUESTION, use_rag=True)
        response = await gemini_client.chat(request)

        if response.success:
            print(f"\nü§ñ AI (—Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤): {response.content}")
        else:
            logger.error(f"–û—à–∏–±–∫–∞: {response.error}")

    finally:
        await vector_store.close()


if __name__ == "__main__":
    asyncio.run(main())